{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example dataset. We create a dataset that is split into a train (data, labels) and a test (test_data, test_labels) dataset with corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = make_classification(n_samples=250, n_features=100, n_informative=20, n_redundant=10, random_state=0, shuffle=False)\n",
    "\n",
    "my_data = pd.DataFrame(data[0])\n",
    "my_target = data[1]\n",
    "my_feat_names = ['f{0}'.format(x+1) for x in range(len(my_data.columns))]\n",
    "\n",
    "data, test_data, labels, test_labels = train_test_split(my_data, my_target, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains code with which we perform best parameter selection, i.e. the best parameter combination for the elastic net models in RENT. This step is NOT REQUIRED for running RENT; parameters can be user defined in a later step of this document. If you play around with RENT and use it the first time, just ignore this cell. \n",
    "It is important to remember that the paramter \"C\" which is responsible for the regularization strength, is inversely defined in the LogisticRegression function of scikit learn. This can be important to know when you want to further interpret results etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import parameter_selection as ps\n",
    "import warnings\n",
    "\n",
    "# Activate this to not show all the convergence warnings.\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "my_reg_params = [0.1,1,10]\n",
    "my_l1_params = [0,0.1,0.25, 0.5, 0.75, 0.9, 1]\n",
    "testsize_range = (0.25, 0.25)\n",
    "\n",
    "best_C, best_l1 = ps.parameter_selection(data=data, labels=labels, \n",
    "                     my_reg_params=my_reg_params, \n",
    "                     my_l1_params=my_l1_params,\n",
    "                     n_splits=5, \n",
    "                     testsize_range = testsize_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"best C: \", best_C)\n",
    "print(\"best l1: \", best_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would be the best parameter combination with the procedure from above. As stated before, this is not necessary but you can select own parameters you would like to try RENT with, directly in RENT. RENT delivers then the best paramter combination found. The process above is useful for speeding up the whole procedure as RENT is then run only with one parameter combination. For the fundamental applcation of RENT see the next cell.\n",
    "\n",
    "RENT offers different settings which are described in the RENT_parallel file. The setting here is the standard setting used in our paper with fewer tt_splits (faster computation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajenul\\OneDrive - Norwegian University of Life Sciences\\GitHub\\RENT\\src\\RENT\\RENT_parallel.py:318: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if(self.clf is not \"RM\" and self.clf is not \"linSVC\"):\n",
      "C:\\Users\\ajenul\\OneDrive - Norwegian University of Life Sciences\\GitHub\\RENT\\src\\RENT\\RENT_parallel.py:318: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if(self.clf is not \"RM\" and self.clf is not \"linSVC\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim data: (175, 100)\n",
      "Dim target (175,)\n",
      "reg param C: [0.1, 1]\n",
      "l1_params: [0.5, 0.9]\n",
      "num TT splits: 20\n",
      "num weight inits: 1\n",
      "data type: <class 'pandas.core.frame.DataFrame'>\n",
      "verbose: 0\n"
     ]
    }
   ],
   "source": [
    "import RENT_parallel as fs\n",
    "# C parameters you would like to try\n",
    "my_reg_params = [0.1, 1]\n",
    "# l1-strengths you would like to try\n",
    "my_l1_params = [0.5, 0.9]\n",
    "\n",
    "analysis = fs.RENT(data=data, \n",
    "                                target=labels,\n",
    "                                feat_names=data.columns,\n",
    "                                reg_params=my_reg_params,\n",
    "                                poly='OFF', \n",
    "                                scoring='f1',\n",
    "                                clf='logreg',\n",
    "                                testsize_range=(0.25, 0.25),\n",
    "                                num_tt=20, \n",
    "                                num_w_init=1,\n",
    "                                l1_params = my_l1_params,\n",
    "                                verbose = 0)\n",
    "\n",
    "\n",
    "analysis.run_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a closer look into different calculations from RENT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = analysis.get_scores_summary_by_regParam()\n",
    "zeroes = analysis.get_average_zero_features()\n",
    "\n",
    "\n",
    "print(scores)\n",
    "print(zeroes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "normed_scores = (scores-np.nanmin(scores.values))/(np.nanmax(scores.values)-np.nanmin(scores.values))\n",
    "normed_zeroes = (zeroes-np.nanmin(zeroes.values))/(np.nanmax(zeroes.values)-np.nanmin(zeroes.values))\n",
    "normed_zeroes = normed_zeroes.astype(\"float\")\n",
    "\n",
    "combi = (normed_scores ** -1 + normed_zeroes ** -1) ** -1\n",
    "print(combi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the combination matrix we see that the combination C = 0.1, l1 = 0.9 has the highest value. We will use it now as the \"best\" combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_combi_row, best_combi_col  =np.where(combi == np.nanmax(combi.values))\n",
    "l1 = combi.index[np.nanmax(best_combi_row)]\n",
    "C = combi.columns[np.nanmin(best_combi_col)]\n",
    "print(\"C: \", C, \"l1: \", l1)\n",
    "\n",
    "summary_spec_weights, sel_feat_df, variables = analysis.get_spec_weights_summary(reg_param=C, l1_param=l1, cutoff_perc=0.9, cutoff_means_ratio=0.9, cutoff_mean_std_ratio=0.975,  sel_approach = \"new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image above shows the counts for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary specific weight shows the summary statistics for each feature\n",
    "summary_spec_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sel_feat_df contains the selected features\n",
    "sel_feat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform a feasibility study -- see paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.feasibility_study(test_data=test_data, test_labels=test_labels, feature_size= len(sel_feat_df.columns), \n",
    "                          features=sel_feat_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test set\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, matthews_corrcoef, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "sc = StandardScaler()\n",
    "train_data_1 = sc.fit_transform(data.loc[:, sel_feat_df.columns])\n",
    "test_data_1 = sc.transform(test_data.loc[:, sel_feat_df.columns])\n",
    "model = LR(penalty='none', max_iter=8000, solver=\"saga\", random_state=0).\\\n",
    "        fit(train_data_1,labels)\n",
    "print(\"All features f1 1: \", f1_score(test_labels, model.predict(test_data_1)))\n",
    "print(\"All features f1 0: \", f1_score(1 - test_labels, 1 - model.predict(test_data_1)))\n",
    "print(\"All features acc: \", accuracy_score(test_labels, model.predict(test_data_1)))\n",
    "print(\"All features matthews: \", matthews_corrcoef(test_labels, model.predict(test_data_1)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the feature selection RENT has the property of summarizing the predictive behavior of single samples. Before we can generate plots for them we also need to check how often they were classified incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc = analysis.get_spec_incorr_lables(0.1, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.confusion_variance_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an overview of the logisic regression predicitons for each sample we need to store them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.pred_proba()\n",
    "analysis.pred_proba_plot(0.1, 0.9, [1,2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
