{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RENT applied to a binary classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook illustrates how to apply RENT to your data for feature selection with a *binary classification* problem. It is also complimentary to the manscript published at arXiv.org.\n",
    "\n",
    "[RENT -- Repeated Elastic Net Technique for Feature Selection](https://arxiv.org/abs/2009.12780)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an example on how to use RENT for feature selection on a **regression problem**, please have a look this [Jupyter Notebook](https://github.com/NMBU-Data-Science/RENT/blob/master/src/RENT/Regression_example.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Load Wisconsin Breast Cancer dataset](#Load-Wisconsin-Breast-Cancer-dataset)\n",
    "2. [Define RENT ensemble for binary classification](#Define-RENT-ensemble-for-binary-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import needed modulse and apply some settings to the Jupyter notebook for better visualisation of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 2000)\n",
    "import RENT\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Wisconsin Breast Cancer dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the data from sciki-learn, store it in a pandas DataFrame and split it into a training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "wisconsin = load_breast_cancer()\n",
    "data = pd.DataFrame(wisconsin.data)\n",
    "data.columns = wisconsin.feature_names\n",
    "target = wisconsin.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, target, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>radius error</th>\n",
       "      <th>texture error</th>\n",
       "      <th>perimeter error</th>\n",
       "      <th>area error</th>\n",
       "      <th>smoothness error</th>\n",
       "      <th>compactness error</th>\n",
       "      <th>concavity error</th>\n",
       "      <th>concave points error</th>\n",
       "      <th>symmetry error</th>\n",
       "      <th>fractal dimension error</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>11.85</td>\n",
       "      <td>17.46</td>\n",
       "      <td>75.54</td>\n",
       "      <td>432.7</td>\n",
       "      <td>0.08372</td>\n",
       "      <td>0.05642</td>\n",
       "      <td>0.026880</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.05715</td>\n",
       "      <td>0.2070</td>\n",
       "      <td>1.2380</td>\n",
       "      <td>1.234</td>\n",
       "      <td>13.88</td>\n",
       "      <td>0.007595</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.014120</td>\n",
       "      <td>0.008578</td>\n",
       "      <td>0.01792</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>13.06</td>\n",
       "      <td>25.75</td>\n",
       "      <td>84.35</td>\n",
       "      <td>517.8</td>\n",
       "      <td>0.1369</td>\n",
       "      <td>0.17580</td>\n",
       "      <td>0.13160</td>\n",
       "      <td>0.09140</td>\n",
       "      <td>0.3101</td>\n",
       "      <td>0.07007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332</td>\n",
       "      <td>11.22</td>\n",
       "      <td>19.86</td>\n",
       "      <td>71.94</td>\n",
       "      <td>387.3</td>\n",
       "      <td>0.10540</td>\n",
       "      <td>0.06779</td>\n",
       "      <td>0.005006</td>\n",
       "      <td>0.007583</td>\n",
       "      <td>0.1940</td>\n",
       "      <td>0.06028</td>\n",
       "      <td>0.2976</td>\n",
       "      <td>1.9660</td>\n",
       "      <td>1.959</td>\n",
       "      <td>19.62</td>\n",
       "      <td>0.012890</td>\n",
       "      <td>0.011040</td>\n",
       "      <td>0.003297</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>0.04243</td>\n",
       "      <td>0.001963</td>\n",
       "      <td>11.98</td>\n",
       "      <td>25.78</td>\n",
       "      <td>76.91</td>\n",
       "      <td>436.1</td>\n",
       "      <td>0.1424</td>\n",
       "      <td>0.09669</td>\n",
       "      <td>0.01335</td>\n",
       "      <td>0.02022</td>\n",
       "      <td>0.3292</td>\n",
       "      <td>0.06522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>0.097910</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>0.7655</td>\n",
       "      <td>2.4630</td>\n",
       "      <td>5.203</td>\n",
       "      <td>99.04</td>\n",
       "      <td>0.005769</td>\n",
       "      <td>0.024230</td>\n",
       "      <td>0.039500</td>\n",
       "      <td>0.016780</td>\n",
       "      <td>0.01898</td>\n",
       "      <td>0.002498</td>\n",
       "      <td>23.69</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.1166</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.32150</td>\n",
       "      <td>0.16280</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>13.59</td>\n",
       "      <td>17.84</td>\n",
       "      <td>86.24</td>\n",
       "      <td>572.3</td>\n",
       "      <td>0.07948</td>\n",
       "      <td>0.04052</td>\n",
       "      <td>0.019970</td>\n",
       "      <td>0.012380</td>\n",
       "      <td>0.1573</td>\n",
       "      <td>0.05520</td>\n",
       "      <td>0.2580</td>\n",
       "      <td>1.1660</td>\n",
       "      <td>1.683</td>\n",
       "      <td>22.22</td>\n",
       "      <td>0.003741</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>0.010650</td>\n",
       "      <td>0.005044</td>\n",
       "      <td>0.01344</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>15.50</td>\n",
       "      <td>26.10</td>\n",
       "      <td>98.91</td>\n",
       "      <td>739.1</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>0.07622</td>\n",
       "      <td>0.10600</td>\n",
       "      <td>0.05185</td>\n",
       "      <td>0.2335</td>\n",
       "      <td>0.06263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>489</td>\n",
       "      <td>16.69</td>\n",
       "      <td>20.20</td>\n",
       "      <td>107.10</td>\n",
       "      <td>857.6</td>\n",
       "      <td>0.07497</td>\n",
       "      <td>0.07112</td>\n",
       "      <td>0.036490</td>\n",
       "      <td>0.023070</td>\n",
       "      <td>0.1846</td>\n",
       "      <td>0.05325</td>\n",
       "      <td>0.2473</td>\n",
       "      <td>0.5679</td>\n",
       "      <td>1.775</td>\n",
       "      <td>22.95</td>\n",
       "      <td>0.002667</td>\n",
       "      <td>0.014460</td>\n",
       "      <td>0.014230</td>\n",
       "      <td>0.005297</td>\n",
       "      <td>0.01961</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>19.18</td>\n",
       "      <td>26.56</td>\n",
       "      <td>127.30</td>\n",
       "      <td>1084.0</td>\n",
       "      <td>0.1009</td>\n",
       "      <td>0.29200</td>\n",
       "      <td>0.24770</td>\n",
       "      <td>0.08737</td>\n",
       "      <td>0.4677</td>\n",
       "      <td>0.07623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "293        11.85         17.46           75.54      432.7          0.08372   \n",
       "332        11.22         19.86           71.94      387.3          0.10540   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "278        13.59         17.84           86.24      572.3          0.07948   \n",
       "489        16.69         20.20          107.10      857.6          0.07497   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "293           0.05642        0.026880             0.022800         0.1875   \n",
       "332           0.06779        0.005006             0.007583         0.1940   \n",
       "565           0.10340        0.144000             0.097910         0.1752   \n",
       "278           0.04052        0.019970             0.012380         0.1573   \n",
       "489           0.07112        0.036490             0.023070         0.1846   \n",
       "\n",
       "     mean fractal dimension  radius error  texture error  perimeter error  \\\n",
       "293                 0.05715        0.2070         1.2380            1.234   \n",
       "332                 0.06028        0.2976         1.9660            1.959   \n",
       "565                 0.05533        0.7655         2.4630            5.203   \n",
       "278                 0.05520        0.2580         1.1660            1.683   \n",
       "489                 0.05325        0.2473         0.5679            1.775   \n",
       "\n",
       "     area error  smoothness error  compactness error  concavity error  \\\n",
       "293       13.88          0.007595           0.015000         0.014120   \n",
       "332       19.62          0.012890           0.011040         0.003297   \n",
       "565       99.04          0.005769           0.024230         0.039500   \n",
       "278       22.22          0.003741           0.005274         0.010650   \n",
       "489       22.95          0.002667           0.014460         0.014230   \n",
       "\n",
       "     concave points error  symmetry error  fractal dimension error  \\\n",
       "293              0.008578         0.01792                 0.001784   \n",
       "332              0.004967         0.04243                 0.001963   \n",
       "565              0.016780         0.01898                 0.002498   \n",
       "278              0.005044         0.01344                 0.001126   \n",
       "489              0.005297         0.01961                 0.001700   \n",
       "\n",
       "     worst radius  worst texture  worst perimeter  worst area  \\\n",
       "293         13.06          25.75            84.35       517.8   \n",
       "332         11.98          25.78            76.91       436.1   \n",
       "565         23.69          38.25           155.00      1731.0   \n",
       "278         15.50          26.10            98.91       739.1   \n",
       "489         19.18          26.56           127.30      1084.0   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "293            0.1369            0.17580          0.13160   \n",
       "332            0.1424            0.09669          0.01335   \n",
       "565            0.1166            0.19220          0.32150   \n",
       "278            0.1050            0.07622          0.10600   \n",
       "489            0.1009            0.29200          0.24770   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  \n",
       "293               0.09140          0.3101                  0.07007  \n",
       "332               0.02022          0.3292                  0.06522  \n",
       "565               0.16280          0.2572                  0.06637  \n",
       "278               0.05185          0.2335                  0.06263  \n",
       "489               0.08737          0.4677                  0.07623  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define RENT ensemble for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The main idea**\n",
    "\n",
    "Using the RENT approach we will train an **ensemble of unique models** based on **unique subsets** of the training data. Since each model is trained on a unique subset, all models will be slightly different from each other and elastic net regularisation **may select different features for each model**. \n",
    "\n",
    "RENT investigates **how consistenly elastic net selects features** across all unique models by analysing distributions of the weight sizes of each feature. Using specific selection criteria $\\tau_1$, $\\tau_2$ and $\\tau_3$ applied to those weight size distributions, we can regulate how aggressively RENT will select features from the full set of features. For more details, please consult the paper mentioned at the top of this Juptyer notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elastic net regularisation**\n",
    "\n",
    "$\\lambda_{enet}(\\beta) = \\gamma [\\alpha\\lambda_1(\\beta)+ (1-\\alpha)\\lambda_2(\\beta)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the average prediction performance of the ensemble may vary for different combinations of $\\gamma$ and $\\alpha$ we pre-define a number of values for each and store them in lists `my_C_params` and `my_l1_ratios` and carry out RENT for each pair-wise combination.\n",
    "\n",
    "**Note** regarding elastic net regularisation term above:\n",
    "\n",
    "* RENT input paramter `C` represents inverse values of $\\gamma$\n",
    "* RENT input parameter `l1_ratio` represents values of $\\alpha$. A `l1_ratio = 1` is equivalent to using L1-regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a range of regularisation parameters C for elastic net. A minimum of at least one value is required.\n",
    "my_C_params = [0.1, 1, 10]\n",
    "\n",
    "# Define a reange of l1-ratios for elastic net.  A minimum of at least one value is required.\n",
    "my_l1_ratios = [0, 0.1, 0.25, 0.5, 0.75, 0.9, 1]\n",
    "\n",
    "# Define setting for RENT\n",
    "analysis = RENT.RENT_Classification(data=train_data, \n",
    "                                    target=train_labels, \n",
    "                                    feat_names=train_data.columns, \n",
    "                                    C=my_C_params, \n",
    "                                    l1_ratios=my_l1_ratios,\n",
    "                                    parameter_selection=True,\n",
    "                                    poly='OFF',\n",
    "                                    testsize_range=(0.25,0.25),\n",
    "                                    scoring='mcc',\n",
    "                                    method='logreg',\n",
    "                                    K=100,\n",
    "                                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis.summary_criteria()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can play around with enet parameter setting...\n",
    "analysis.get_enet_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.set_enet_params(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.get_enet_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.set_enet_params(0.1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contain only one element as we did paramter selection beforehand...\n",
    "analysis.get_enetParam_matrices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.get_object_probabilities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.plot_object_probabilities(object_id=[293,332])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.get_weight_distributions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.summary_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_vars = analysis.selectFeatures(tau_1=0.9, tau_2=0.9, tau_3=0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.plot_selection_frequency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.get_runtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.plot_object_PCA(group=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.plot_object_PCA(group=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis.plot_object_PCA(group='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predit test data\n",
    "\n",
    "# Import what is needed for prediction and evaluation of predictions from test set\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, matthews_corrcoef, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "# Scale the data accordingly\n",
    "sc = StandardScaler()\n",
    "train_data_1 = sc.fit_transform(train_data.iloc[:, fs_vars])\n",
    "test_data_1 = sc.transform(test_data.iloc[:, fs_vars])\n",
    "\n",
    "# Train model with \n",
    "model = LR(penalty='none', max_iter=8000, solver=\"saga\", random_state=0).\\\n",
    "        fit(train_data_1, train_labels)\n",
    "\n",
    "# Print results\n",
    "print(\"f1 1: \", f1_score(test_labels, model.predict(test_data_1)))\n",
    "print(\"f1 0: \", f1_score(1 - test_labels, 1 - model.predict(test_data_1)))\n",
    "print(\"Accuracy: \", accuracy_score(test_labels, model.predict(test_data_1)))\n",
    "print(\"Matthews correlation coefficient: \", matthews_corrcoef(test_labels, model.predict(test_data_1)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
